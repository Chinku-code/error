Perfect â€” thanks for sharing the **actual Dockerfile** ğŸ‘
Now Iâ€™ll **surgically fix it** (no unnecessary changes) and give you the **correct YAMLs** that will stop the **NotReady / restart loop** permanently.

---

# ğŸ”´ Root cause (recap, based on YOUR Dockerfile)

Your container runs:

```bash
spark-submit /app/app.jar
```

Spark **finishes successfully** â†’ container **exits** â†’
Deployment thinks container died â†’ **restarts pod** â†’ **NotReady loop**

âœ” Spark is behaving correctly
âŒ Deployment type is wrong for a batch job

---

# âœ… WHAT Iâ€™M CHANGING (minimal & correct)

1. **Keep your base image**
2. **Move data paths to writable volume (`/data`)**
3. **Use Job instead of Deployment**
4. **Keep `spark-submit` (no hacks like `sleep`)**

---

# 1ï¸âƒ£ UPDATED DOCKERFILE (based on yours)

### âœ… Correct & OpenShift-safe

```dockerfile
FROM registry.dev.sbiepay.sbi:8443/spark/sparkrecon:4.0.0_12092025v43

WORKDIR /app

# Spark temp artifacts
ENV SPARK_SUBMIT_OPTS="-Dspark.sql.artifact.dir=/tmp/spark-artifacts"

# Create writable directories
RUN mkdir -p /data/input_files \
             /data/output_files \
             /tmp/spark-artifacts \
    && chmod -R 777 /data /tmp

# Copy application JAR
COPY spring-context-spark-1.0.jar /app/app.jar

# Copy CSV (only if static input)
COPY 1766041567850-1000003_Transaction_spark_2.csv /data/input_files/

# Run Spark batch job
ENTRYPOINT ["spark-submit"]
CMD ["/app/app.jar"]
```

### ğŸ”‘ Why this is correct

* `/app` â†’ image layer (read-only in OpenShift)
* `/data` â†’ PVC-mounted, writable
* No fake `sleep` / `tail -f` hacks
* Spark exits â†’ Job completes

---

# 2ï¸âƒ£ PVC YAML (REQUIRED)

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: spark-pvc
  namespace: dev-rns
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```

Apply once:

```bash
oc apply -f spark-pvc.yaml
```

---

# 3ï¸âƒ£ ğŸ”¥ REPLACE Deployment WITH Job (MOST IMPORTANT)

ğŸš« **DELETE Deployment**

```bash
oc delete deploy spark-v9 -n dev-rns
```

---

## âœ… Spark Job YAML (FINAL & CORRECT)

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: spark-v9
  namespace: dev-rns
spec:
  backoffLimit: 0
  template:
    metadata:
      labels:
        app: spark-v9
    spec:
      restartPolicy: Never
      containers:
      - name: spark-v9
        image: image-registry.openshift-image-registry.svc:5000/dev-rns/spark-v9:v9

        env:
        - name: SPARK_SUBMIT_OPTS
          value: "-Dspark.sql.artifact.dir=/tmp/spark-artifacts"

        volumeMounts:
        - name: spark-data
          mountPath: /data

      volumes:
      - name: spark-data
        persistentVolumeClaim:
          claimName: spark-pvc
```

Apply:

```bash
oc apply -f spark-job.yaml
```

---

# 4ï¸âƒ£ REQUIRED Spark Code Path Change

### âŒ CURRENT (problematic)

```
file:///app/1766041567850-1000003_Transaction_spark_2.csv
file:///app/output_files
```

### âœ… FIX

```
file:///data/input_files/1766041567850-1000003_Transaction_spark_2.csv
file:///data/output_files
```

Example:

```java
spark.read()
     .option("header", true)
     .csv("/data/input_files/1766041567850-1000003_Transaction_spark_2.csv");

df.write()
  .mode("overwrite")
  .csv("/data/output_files");
```

---

# 5ï¸âƒ£ How to verify (important)

```bash
oc get jobs -n dev-rns
oc get pods -n dev-rns
oc logs job/spark-v9 -n dev-rns
```

### âœ… Expected

```
STATUS: Completed
```

âœ” No NotReady
âœ” No restart
âœ” Output persists
âœ” Spark exits cleanly

---

# ğŸ¯ FINAL SUMMARY

| Problem            | Fixed By               |
| ------------------ | ---------------------- |
| NotReady loop      | Deployment â†’ Job       |
| Read-only FS risk  | PVC + `/data`          |
| Spark exit restart | `restartPolicy: Never` |
| Data loss          | PVC                    |

---

If you want next:

* ğŸ” **CronJob** (run every X minutes)
* ğŸ“Š **Spark History Server**
* ğŸš€ **Executor tuning (memory / CPU)**
* ğŸ” **SecurityContext for restricted SCC**

Just tell me whatâ€™s next ğŸ‘Œ
